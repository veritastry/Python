scrapy框架：

reqeust下载页面
解析
并发

深度爬虫
中间有一个引擎
去scheduler里面吧这个url拿过来（队列）
去下载器拿到这个url
我们只需要做spider就可以了，一个会去调度器（取这个url），一个会去持久化（保存数据）
下载器是下载对应的url

解析响应内容：
给调度器





创建一个scrapy项目：
scrapy startproject scrapy_app01 
cd  scrapy_app01
scrapy genspider chouti  chouti.com
打开chouti.oy 进行编辑
scrapy crawl chouti  --nolog




##SPIDER_MIDDLEWARES = {}



如果是window时，并且出现编码问题时，可以加
sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding='gb18030')






# -*- coding: utf-8 -*-
import scrapy


class ChoutiSpider(scrapy.Spider):
    name = 'chouti'
    allowed_domains = ['chouti.com']###########支持的域名是这个 
    start_urls = ['http://chouti.com/']###############起始url


#######################解析
    def parse(self, response):
        print(response)
	pritn(response.url)这个会访问这个地址是什么
	print(response.body)
























































































































































